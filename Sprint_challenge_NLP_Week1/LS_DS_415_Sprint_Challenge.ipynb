{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset/challenge). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "*Successfully complete these all these objectives to earn a 2. There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yelp = pd.read_json('review_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining stop_words\n",
    "\n",
    "stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do tokenization on test\n",
    "\n",
    "def tokenize(docs):\n",
    "    for tokens in docs:\n",
    "        \n",
    "        text = [tokens.lemma_ for tokens in nlp(docs) if\n",
    "               (tokens.is_punct == False) and\n",
    "               (tokens.text.lower() not in stop_words)]\n",
    "        \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samsung\\Anaconda3\\envs\\unit4-NLP\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# import progress apply function\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# testing to make sure function works. \n",
    "\n",
    "test_run = yelp['text'][:5].progress_apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [beware, fake, fake, fake, small, business, Lo...\n",
       "1    [come, lunch, Togo, service, quick, staff, fri...\n",
       "2    [Vegas, dozen, time, step, foot, Circus, Circu...\n",
       "3    [go, night, close, street, party, good, actual...\n",
       "4    [3.5, 4, star, \\n\\n, bad, price, $, 12.99, lun...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beware',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'small',\n",
       " 'business',\n",
       " 'Los',\n",
       " 'Alamitos',\n",
       " 'receive',\n",
       " 'look',\n",
       " 'like',\n",
       " 'legitimate',\n",
       " 'bill',\n",
       " '$',\n",
       " '70',\n",
       " 'account',\n",
       " 'number',\n",
       " ' ',\n",
       " 'call',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'list',\n",
       " '866',\n",
       " '273',\n",
       " '7934',\n",
       " ' ',\n",
       " 'wait',\n",
       " 'time',\n",
       " 'hold',\n",
       " 'say',\n",
       " '20',\n",
       " 'minute',\n",
       " 'leave',\n",
       " 'message',\n",
       " ' ',\n",
       " 'live',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'matter',\n",
       " 'number',\n",
       " 'select',\n",
       " ' ',\n",
       " 'leave',\n",
       " 'firm',\n",
       " 'message',\n",
       " 'contact',\n",
       " 'BBB',\n",
       " 'attorney',\n",
       " 'company',\n",
       " 'try',\n",
       " 'scam',\n",
       " 'business',\n",
       " 'illegal']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_run[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, it will probably be best to use a `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a TF-IDF feature matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfdif_vector(docs):\n",
    "    # instantiate vectorizer\n",
    "\n",
    "    tfdf = TfidfVectorizer(stop_words = 'english',\n",
    "                           ngram_range = (1,2),\n",
    "                           min_df = 3,\n",
    "                           max_df = 0.5)\n",
    "\n",
    "    # create a vocabulary and tf-idf score per document\n",
    "\n",
    "    tf_score = tfdf.fit_transform(docs)\n",
    "\n",
    "    # get feature names to use as dataframe column headers\n",
    "\n",
    "    tf_score = pd.DataFrame(tf_score.todense(), \n",
    "                            columns = tfdf.get_feature_names())\n",
    "    \n",
    "    shape = print(f'tf_score shape = {tf_score.shape}')\n",
    "    \n",
    "    head = tf_score.head()\n",
    "    \n",
    "    return shape, head, tf_score, tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_score shape = (10000, 31696)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "     00  00 extra  00 pm  000  00am  00pm   01   04   05   06  ...  zucchini  \\\n",
       " 0  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       " 1  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       " 2  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       " 3  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       " 4  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       " \n",
       "    zucchini fries  zumba   ça  équipe  érable  était  était très  été  être  \n",
       " 0             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       " 1             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       " 2             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       " 3             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       " 4             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       " \n",
       " [5 rows x 31696 columns],\n",
       "        00  00 extra  00 pm  000      00am  00pm   01   04   05   06  ...  \\\n",
       " 0     0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 1     0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 2     0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 3     0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 4     0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " ...   ...       ...    ...  ...       ...   ...  ...  ...  ...  ...  ...   \n",
       " 9995  0.0       0.0    0.0  0.0  0.163167   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 9996  0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 9997  0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 9998  0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 9999  0.0       0.0    0.0  0.0  0.000000   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " \n",
       "       zucchini  zucchini fries  zumba   ça  équipe  érable  était  était très  \\\n",
       " 0          0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " 1          0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " 2          0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " 3          0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " 4          0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " ...        ...             ...    ...  ...     ...     ...    ...         ...   \n",
       " 9995       0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " 9996       0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " 9997       0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " 9998       0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " 9999       0.0             0.0    0.0  0.0     0.0     0.0    0.0         0.0   \n",
       " \n",
       "       été  être  \n",
       " 0     0.0   0.0  \n",
       " 1     0.0   0.0  \n",
       " 2     0.0   0.0  \n",
       " 3     0.0   0.0  \n",
       " 4     0.0   0.0  \n",
       " ...   ...   ...  \n",
       " 9995  0.0   0.0  \n",
       " 9996  0.0   0.0  \n",
       " 9997  0.0   0.0  \n",
       " 9998  0.0   0.0  \n",
       " 9999  0.0   0.0  \n",
       " \n",
       " [10000 rows x 31696 columns])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the text column from yelp dataset. \n",
    "\n",
    "# testing to see if the function works\n",
    "\n",
    "tfdif_vector(yelp['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_score shape = (10000, 31696)\n"
     ]
    }
   ],
   "source": [
    "# using the function above but this time assigning variables to it\n",
    "\n",
    "tf_shape, tf_head, doc_vector, tfdf = tfdif_vector(yelp['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
       "                min_df=3, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying that tfdf is a standalone object that can be called.\n",
    "\n",
    "tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 31696)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 extra</th>\n",
       "      <th>00 pm</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>...</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zucchini fries</th>\n",
       "      <th>zumba</th>\n",
       "      <th>ça</th>\n",
       "      <th>équipe</th>\n",
       "      <th>érable</th>\n",
       "      <th>était</th>\n",
       "      <th>était très</th>\n",
       "      <th>été</th>\n",
       "      <th>être</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  00 extra  00 pm  000  00am  00pm   01   04   05   06  ...  zucchini  \\\n",
       "0  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "1  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "2  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "3  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "4  0.0       0.0    0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "\n",
       "   zucchini fries  zumba   ça  équipe  érable  était  était très  été  être  \n",
       "0             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       "1             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       "2             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       "3             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       "4             0.0    0.0  0.0     0.0     0.0    0.0         0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 31696 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(doc_vector.shape)\n",
    "doc_vector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a nearest neighbor model. \n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# fit on vectorized text\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
    "\n",
    "nn.fit(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample review. This is a real review from yelp\n",
    "\n",
    "review = [\"\"\"\n",
    "\n",
    "My boyfriend and I went here for the \n",
    "first time and I wouldn't be surprised \n",
    "if we went back today. It's that good. \n",
    "The tea selection is vast and they all \n",
    "looked quite tasty! They have milk-teas, \n",
    "regular teas, boba-teas... you name it, \n",
    "they got it! It seemed to be made to order, \n",
    "so everything was fresh!\n",
    "They also have a large selection of cute \n",
    "snacks, cakes, and mochis. It all looked \n",
    "so tasty and fun. I got a chocolate mousse \n",
    "that is shaped like an ADORABLE dog and not \n",
    "only did it taste delicious, it was really \n",
    "really cool looking.\n",
    "Everything is moderately priced and it is\n",
    "an incredible experience all around.\n",
    "The store has an awesome, open, and airy \n",
    "ambiance. When COVID restrictions are over, \n",
    "I would totally come here to do work. \n",
    "The store is clean and they are practicing \n",
    "social distancing like champs.\n",
    "I would highly suggest coming here ASAP. \n",
    "It's almost too good to be true.\n",
    "The cups they serve the drinks in are pretty epic. \n",
    "They have a cool top that doesn't allow for the drink to spill.\n",
    "\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query of similar document to the one posted above. \n",
    "\n",
    "# first, vectorize the sample review above. \n",
    "\n",
    "query = tfdf.transform(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nearest neighbor to find documents in the original yelp\n",
    "# review that is similar to the review above. \n",
    "\n",
    "# nearest neighbor doesn't work on sparse matrix which is what query is\n",
    "# first, convert the spart matrix to dense. \n",
    "\n",
    "dense = query.todense()\n",
    "\n",
    "similar = nn.kneighbors(dense, return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6311],\n",
       "       [6204],\n",
       "       [7164],\n",
       "       [1754],\n",
       "       [4295],\n",
       "       [2443],\n",
       "       [9909],\n",
       "       [5305],\n",
       "       [6987],\n",
       "       [2862]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now print the documents from the original yelp review that is similar\n",
    "# to the sample review above. \n",
    "\n",
    "# these are the top 10 reviews that are similar to the sample review above\n",
    "\n",
    "similar.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This place has really good boba! They offer a lot of flavors for drinks and they don't use artificial flavoring when it comes to the fruit smoothies. You can add more than just boba in your drinks and the lady at the front always gives suggestions on what you should order based on your preference. The boba has a good consistency and it's not over cooked. Their ice teas are really good and the milk teas as well. Overall a really good place to get boba drinks.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The third prediction makes perfect sense because this review \n",
    "# i grabbed from yelp was actually of a boba tea shop. INTERESTING. \n",
    "\n",
    "# The first 2 reviews in the prediction was in a different language. \n",
    "# That's interesting too. \n",
    "\n",
    "yelp['text'][7164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天氣很熱吃不下東西，今天我點了一個韓國冷面湯、餐後點了甜點，冰沙系列不會太甜膩，覺得店家很用心製作，包含擺盤精緻、佐料衛生，夏日想開胃，這是一個不錯的選擇，服務人員也很敬業，以後會常常來 \n",
      "\n",
      " 旅行でラスベガスに来ましたがネイルがはげてるのが気になり、探したお店でした。\n",
      "質問にも丁寧に答えてくれましたし、日本人の方も日本語が話せる方も居て、とても綺麗で居心地のいいお店でした。 \n",
      "ネイルはちはるさんと言う綺麗な方が丁寧にしてくれとても気に入りました。\n",
      "予定になかったまつ毛エクステもお願いし、日本ではまだあまりないブラウンカラーのエクステをしてもらい、とても気に入りました。\n",
      "また是非マッサージなどで伺いたいと思います。 \n",
      "\n",
      " This place has really good boba! They offer a lot of flavors for drinks and they don't use artificial flavoring when it comes to the fruit smoothies. You can add more than just boba in your drinks and the lady at the front always gives suggestions on what you should order based on your preference. The boba has a good consistency and it's not over cooked. Their ice teas are really good and the milk teas as well. Overall a really good place to get boba drinks. \n",
      "\n",
      " A ok for sure!! Pretty decent Mexican food in a cutesy little area. The lunch combos were a bit high for this type of joint, around $9. \n",
      "\n",
      "The chips and salsa were great and service rwas friendly but slow. \n",
      "\n",
      "Good solid choice if you're out and about over here \n",
      "\n",
      " This place is wonderful... one of my favorite places in Montreal for sure!\n",
      "\n",
      "Both the teahouse and the boutique are heaven for tea lovers. The teahouse has a peaceful, intimate atmosphere with gentle lighting, a lot of wood and natural materials, and Asian-influenced decor. This is not a fast-paced, grab a cup and run type of shop, it's a place to take a pause from your day, slow down and savor a delicious tea.\n",
      "\n",
      "The tea selection is amazing, offering a wide range of options, black, green, chai, matcha, whatever your preference. With some teas you have a choice of how it's served, e.g., gaiwan or teapot. The staff are wonderfully knowledgeable and helpful. Of course the tea is top-quality.\n",
      "\n",
      "I strongly recommend browsing the boutique as well. They have an incredible selection of teas from Chinese pu-erh to Japanese green, a lovely selection of teaware, books, and gifts.\n",
      "\n",
      "Like I said, heaven for tea lovers!\n",
      "\n",
      "One piece of advice: large groups might be difficult here. Four small-to-regular sized people can just barely fit into a booth. I wouldn't recommend any more than that. \n",
      "\n",
      " It seems totally unbelievable that, not only did I hang out for more than an hour at a mall, but an outside one at that in 115-degree heat. I attribute my ability to withstand these two unfavorable conditions to two things:\n",
      "\n",
      "Bonnie \n",
      "Tea Infusion \n",
      " \n",
      "\n",
      "It was Bonnie's choice to grab some drinks at what she described as a \"great, independently owned tea shop\" located at Tempe Marketplace. I'm not a tea connoisseur by any means, and I generally distrust fads (i.e., this silly-sounding thing called \"boba\" tea), but I agreed cause if  Bonnie likes it, it's gotta be worth checking out, right? \n",
      "\n",
      "\n",
      "Parking wasn't too much of a hassle on a Sunday afternoon, and we surprisingly got a space close to an entrance to the \"District,\" just outside of Barnes and Noble. Inside, the shop looked much like a Starbucks - and I don't mean to imply that it was uninspired or anything. It was clean, modern and consisted of a long counter behind which two employees busily hand-mixed tea and smoothie concoctions, along with a few tables/chairs. \n",
      "\n",
      "\n",
      "The menu was extensive and separated into \"black,\" \"green\" and \"white\" teas, any of which I could get hot or cold...I think. I picked the tropics white in iced form, and B got my second choice, a peach mix white. Both were pretty delicious. There was also a selection of sandwiches and snacks that I did not sample - next time. I also ran into a good friend there who was delighted by the fact that they had this mystical \"boba,\" and so ordered a Chai-type version with blueberry-sized and -shaped balls floating on the bottom, and an oversized straw to facilitate sucking them up along with the drink. I tried one boba ball and it was gelatinous and chewy - not at all like what I expected - and an odd sensation, but I would totally order one the next time I'm in. \n",
      "\n",
      "\n",
      "Being in the District and flanked by stores like \"Hottie Inc.\" and Guess, there were several teens inside hanging out in large numbers - boo. So, we decided to sit outside for as long as we could stand it. I was surprised to find that the several shaded tables and misters made for a pretty comfortable environment, and we hung out for a good hour or so before I got so sweaty I have to get mobile.  \n",
      "\n",
      "\n",
      "So, in summary, I would totally come back. Probably before or after a movie, to grab a drink and support an independent store among a mecca of chain retail. Plus, right across the way there is an ASU art gallery (closed Sunday and Monday) that we peeked into and actually cool, interesting sculpture art by local artist and asu instructor David Young.  \n",
      "\n",
      "\n",
      "Touche, the District at Tempe Marketplace - I have to say I am impressed! \n",
      "\n",
      " What a great boba place this is\n",
      "Upon walking in, hearing the chines(Throwback to my old boba place) and seeing all the decorations , i knew I was going to get great boba. All the decorations fit well with the theme the boba place has and kinda cute too. I ordered \"Hokkaido Milk Tea\" Regular with boba, it came out at $4+ with large being $5+. After getting my milk tea, it tasted really nice and flavorful, the boba being very chewy and not hard at all. All the tables in the store are pretty clean and clean floors too. I really like this boba place and would recommend to a friend to visit, kinda wish the price was lower so I can order large milk tea with boba being under $5...\n",
      "\n",
      "TLDR: Great atmosphere, great Boba, and OK service, high prices tho... \n",
      "\n",
      " Omg.....Brew Tea Bar....omg....\n",
      "\n",
      "My first visit....I had to get a milk tea and a slushie.....lol\n",
      "\n",
      "Since it was my first time....I figured I would try their \"The Brew\" Milk Tea since it was their special mix.  And it did not disappoint. It was DELICIOUS! \n",
      "\n",
      "I love milk tea and have had it at multiple places....and BTB completely rocks it!!!  And if you like Egg Pudding. . .get it here.  It is the absolute best Egg Pudding that I have ever had. . .it is sooooo silky and totally just melts away when you slurp it up with the tea.  =)  Totally delish!\n",
      "\n",
      "Since my first visit. I have been there too many times to count. . .and everthing that I have tried from all the different teas to the COLD Brew coffee with their salted foam is sooooo freaking good!!!  I wake up craving the teas from here. . .=)\n",
      "\n",
      "I am sooo glad that they opened up so close to my home. . .I might as well just give them my paycheck!  LOL  Yes they are that good and I want to go to them to get tea multiple times a week!  \n",
      "\n",
      "They are crazy good!!!!  I would highly reccomend them to anyone that is craving some awesome delicious tea. . .or just want to try something new!  =)  \n",
      "\n",
      "Seriously, just get one. . .you won't regret it!!!  =) \n",
      "\n",
      " On the way out? You may have heard that Starbucks is shutting down all Teavana locations. That's unfortunate because it's my favorite place to go for loose-leaf teas of every imaginable variety. This location is no exception.\n",
      "\n",
      "While the prices are steep (get it? it's a tea joke), I don't mind too much. There's literally no other local place you can find loose leaf tea. (Maybe you can at some super-specialty hoity-toity organic grocers.) That's my favorite point here. They have teas of all possible kinds with a few different varieties within the categories too. White, green, black, herbal, chai, etc. There's even the sought after monkey-picked tea. My favorites are in the green and white tea categories.\n",
      "\n",
      "The expensive items in this store include the incredibly overpriced teapots. You can purchase the exact same thing at World Market for about 1/10 the price, no kidding. $20 there, $200 here. There are lots of tea sets, storage containers, brewing jugs and portable tumblers. It seems most of their stuff is made of metal, stone or glass which is great really! But again, it's hard to get over those prices.\n",
      "\n",
      "Last but not least, the service here is a mixed bag. You can try one of the taster teas at the entryway and order a whole cup of it but last time I did it took forever to make because two shopkeepers were doing busy work while just one was making all of the drinks in order. \n",
      "\n",
      " I was so disappointed when Sterling Social closed down but thrilled when this absolute gem opened up in its place! The coffee is fantastic, and I had an incredible roasted beet sandwich there today. The food all looks SO GOOD that I'm going to come by and try the soups (which are apparently all vegan) and egg cups soon. \n",
      "\n",
      "Not only is the staff all ridiculously friendly, I really, really like all of them. They just seem like really cool people. \n",
      "\n",
      "The interior is beautiful, bright, and open: a HUGE improvement on the Sterling Social. I can't wait to spend an afternoon here with my laptop and spend the afternoon eating.\n"
     ]
    }
   ],
   "source": [
    "# All predicted similar reviews\n",
    "\n",
    "print(yelp['text'][6311], \"\\n\\n\", \n",
    "      yelp['text'][6204], \"\\n\\n\",\n",
    "      yelp['text'][7164], \"\\n\\n\",\n",
    "      yelp['text'][1745], \"\\n\\n\",\n",
    "      yelp['text'][4295], \"\\n\\n\",\n",
    "      yelp['text'][2443], \"\\n\\n\",\n",
    "      yelp['text'][9909], \"\\n\\n\",\n",
    "      yelp['text'][5305], \"\\n\\n\",\n",
    "      yelp['text'][6987], \"\\n\\n\",\n",
    "      yelp['text'][2862])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a piepline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier. Use that pipeline to estimate a model to predict `stars`. Use the Pipeline to predict a star rating for your fake review from Part 2. \n",
    "2. Tune the entire pipeline with a GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate random forest classifier\n",
    "\n",
    "random = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pipeline\n",
    "# use the vectorizer already defined in the tfdif vector function\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', tfdf),\n",
    "    ('clf', random)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test dataset. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(yelp['text'], \n",
    "                                                    yelp['stars'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=yelp['stars'],\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.5, max_features=None,\n",
       "                                 min_df=3, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_patte...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the pipeline to the training dataset. \n",
    "\n",
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.562"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy score\n",
    "\n",
    "y_pred = pipe.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the pipeline above to predict star rating for sample review\n",
    "\n",
    "# our pipeline gives us a prediction of 4. It was actually 5. \n",
    "\n",
    "pipe.predict(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=0.5,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=3,\n",
       "                                                        ngram_range=(1, 2),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words='english',\n",
       "                                                        strip...\n",
       "                                                               n_jobs=None,\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'clf__max_depth': (10, 50, None),\n",
       "                         'clf__n_estimators': (5, 20),\n",
       "                         'vect__max_df': (0.5, 1.0),\n",
       "                         'vect__max_features': (500, 10000),\n",
       "                         'vect__min_df': (2, 5, 10)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use gridsearch cv to find the optimal parameters for our model\n",
    "\n",
    "param = {\n",
    "    'vect__max_df': (0.5, 1.0),\n",
    "    'vect__min_df': (2, 5, 10),\n",
    "    'vect__max_features': (500, 10000),\n",
    "    'clf__n_estimators': (5, 20),\n",
    "    'clf__max_depth': (10, 50, None)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param, cv = 5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.570125"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_scores from grid_search cf\n",
    "\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__max_depth': None,\n",
       " 'clf__n_estimators': 20,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': 10000,\n",
       " 'vect__min_df': 2}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best parameters from grid_search cv. \n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " new prediction [4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5685"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-doing the prediction with the best parameters above. \n",
    "\n",
    "random2 = RandomForestClassifier(max_depth = None,\n",
    "                               n_estimators = 20)\n",
    "\n",
    "tfdf2 = TfidfVectorizer(stop_words = 'english',\n",
    "                           ngram_range = (1,2),\n",
    "                           min_df = 2,\n",
    "                           max_features = 10000)\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', tfdf2),\n",
    "    ('clf', random2),\n",
    "])\n",
    "\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "# our prediction using new parametetrs\n",
    "print (f'New star prediction {pipe.predict(review)}')\n",
    "\n",
    "# accuracy score\n",
    "y_pred = pipe.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samsung\\Anaconda3\\envs\\unit4-NLP\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "## Your code \n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This text document is 10,000 rows long. It will take a CENTURY to tokenize all of it. So, For this part of the assignment, I will use a sample of 25 rows/documents instead of the entire document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>QE1tzFyyyRKV1p7C4LRukQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-15 02:41:12</td>\n",
       "      <td>0</td>\n",
       "      <td>pcpmloUy9OSbQCxG-NIXKA</td>\n",
       "      <td>1</td>\n",
       "      <td>We ordered 1/4 chicken leg and thigh take out,...</td>\n",
       "      <td>1</td>\n",
       "      <td>AnHGoMsQ62-cUh1pl4bG9g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>wghDrzcZ0VloAtaIZ7GEBg</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-05-05 22:23:44</td>\n",
       "      <td>0</td>\n",
       "      <td>-Gb3R1WOoa_fPpsQ91rLCg</td>\n",
       "      <td>4</td>\n",
       "      <td>nice hike...keeps going up and up and up ...an...</td>\n",
       "      <td>0</td>\n",
       "      <td>7ziWZULyiZv2TesYNMFf4g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>78qDAXntkxbmbiMdfbUJiw</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-10-01 22:04:55</td>\n",
       "      <td>0</td>\n",
       "      <td>uBlyX9pkMmp9evijZtuQ6g</td>\n",
       "      <td>3</td>\n",
       "      <td>I have nothing against trainee teachers.. but ...</td>\n",
       "      <td>2</td>\n",
       "      <td>CxDOIDnH8gp9KXzpBHJYXw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>MtjOk7u7sp3yWyzscAIG4Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-14 01:56:44</td>\n",
       "      <td>0</td>\n",
       "      <td>fHwnCTCeXE5HWdTvGR1LZg</td>\n",
       "      <td>4</td>\n",
       "      <td>My boyfriend and I went this evening for dinne...</td>\n",
       "      <td>0</td>\n",
       "      <td>bVBLGgj0TMKoSRtcZv6_Bw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>KTPRYqiFdLowAUEAnN7e3g</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-02-23 23:04:31</td>\n",
       "      <td>0</td>\n",
       "      <td>ZlUvD0BCVNoB4QHarlTcYA</td>\n",
       "      <td>2</td>\n",
       "      <td>Waitresses dressed like Hooters servers. Burge...</td>\n",
       "      <td>3</td>\n",
       "      <td>TSFRIumlH-tq0hKh9YyhVg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id  cool                date  funny  \\\n",
       "6252  QE1tzFyyyRKV1p7C4LRukQ     0 2015-07-15 02:41:12      0   \n",
       "4684  wghDrzcZ0VloAtaIZ7GEBg     0 2012-05-05 22:23:44      0   \n",
       "1731  78qDAXntkxbmbiMdfbUJiw     3 2010-10-01 22:04:55      0   \n",
       "4742  MtjOk7u7sp3yWyzscAIG4Q     0 2013-06-14 01:56:44      0   \n",
       "4521  KTPRYqiFdLowAUEAnN7e3g     1 2013-02-23 23:04:31      0   \n",
       "\n",
       "                   review_id  stars  \\\n",
       "6252  pcpmloUy9OSbQCxG-NIXKA      1   \n",
       "4684  -Gb3R1WOoa_fPpsQ91rLCg      4   \n",
       "1731  uBlyX9pkMmp9evijZtuQ6g      3   \n",
       "4742  fHwnCTCeXE5HWdTvGR1LZg      4   \n",
       "4521  ZlUvD0BCVNoB4QHarlTcYA      2   \n",
       "\n",
       "                                                   text  useful  \\\n",
       "6252  We ordered 1/4 chicken leg and thigh take out,...       1   \n",
       "4684  nice hike...keeps going up and up and up ...an...       0   \n",
       "1731  I have nothing against trainee teachers.. but ...       2   \n",
       "4742  My boyfriend and I went this evening for dinne...       0   \n",
       "4521  Waitresses dressed like Hooters servers. Burge...       3   \n",
       "\n",
       "                     user_id  \n",
       "6252  AnHGoMsQ62-cUh1pl4bG9g  \n",
       "4684  7ziWZULyiZv2TesYNMFf4g  \n",
       "1731  CxDOIDnH8gp9KXzpBHJYXw  \n",
       "4742  bVBLGgj0TMKoSRtcZv6_Bw  \n",
       "4521  TSFRIumlH-tq0hKh9YyhVg  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample dataset. \n",
    "\n",
    "df = yelp.sample(random_state = 42, n=25)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [11:22<00:00, 27.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>QE1tzFyyyRKV1p7C4LRukQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-15 02:41:12</td>\n",
       "      <td>0</td>\n",
       "      <td>pcpmloUy9OSbQCxG-NIXKA</td>\n",
       "      <td>1</td>\n",
       "      <td>We ordered 1/4 chicken leg and thigh take out,...</td>\n",
       "      <td>1</td>\n",
       "      <td>AnHGoMsQ62-cUh1pl4bG9g</td>\n",
       "      <td>[order, 1/4, chicken, leg, thigh, totally, bur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id  cool                date  funny  \\\n",
       "6252  QE1tzFyyyRKV1p7C4LRukQ     0 2015-07-15 02:41:12      0   \n",
       "\n",
       "                   review_id  stars  \\\n",
       "6252  pcpmloUy9OSbQCxG-NIXKA      1   \n",
       "\n",
       "                                                   text  useful  \\\n",
       "6252  We ordered 1/4 chicken leg and thigh take out,...       1   \n",
       "\n",
       "                     user_id  \\\n",
       "6252  AnHGoMsQ62-cUh1pl4bG9g   \n",
       "\n",
       "                                                 tokens  \n",
       "6252  [order, 1/4, chicken, leg, thigh, totally, bur...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a column to yelp reviews that has the tokens\n",
    "\n",
    "df['tokens'] = df['text'].progress_apply(tokenize)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary representation of all the words\n",
    "\n",
    "dct = corpora.Dictionary(df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words representation of the corpus\n",
    "\n",
    "corpus = [dct.doc2bow(text) for text in df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying LDA to document. \n",
    "\n",
    "lda = LdaMulticore(corpus=corpus,\n",
    "                  id2word = dct,\n",
    "                  random_state = 42,\n",
    "                  num_topics=5,\n",
    "                  passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "  service food \n",
      " noise part\n",
      "\n",
      "------ Topic 1 ------\n",
      "\n",
      "\n",
      " place leak start good get\n",
      "\n",
      "------ Topic 2 ------\n",
      "\n",
      "\n",
      " good like   try customer\n",
      "\n",
      "------ Topic 3 ------\n",
      "order special restaurant tofu minimum food\n",
      "\n",
      "------ Topic 4 ------\n",
      "  great time service dress good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# making the resulting topics look pretty \n",
    "\n",
    "import re\n",
    "\n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in lda.print_topics()]\n",
    "topics = [' '.join(t[0:6]) for t in words]\n",
    "\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating visual interpretation of LDA topics\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1459619538991716562798368025\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1459619538991716562798368025_data = {\"mdsDat\": {\"x\": [0.052502028957096214, -0.14380257705554428, 0.08645048686250385, 0.01519422591301168, -0.010344164677067547], \"y\": [-0.12798405336576357, 0.014704224137615325, 0.10051654479974913, 0.010817012127117453, 0.0019462723012816957], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [37.02375030517578, 25.46895408630371, 23.649953842163086, 11.192326545715332, 2.6650187969207764]}, \"tinfo\": {\"Term\": [\" \", \"special\", \"restaurant\", \"order\", \"food\", \"great\", \"get\", \"find\", \"$\", \"noise\", \"amazing\", \"leak\", \"start\", \"dress\", \"definitely\", \"cold\", \"service\", \"tasty\", \"part\", \"vehicle\", \"time\", \"12\", \"evening\", \"reasonable\", \"minimum\", \"tofu\", \"price\", \"try\", \"actually\", \"away\", \"try\", \"price\", \"wine\", \"store\", \"bootie\", \"family\", \"Troutman\", \"cook\", \"excellent\", \"Nordstrom\", \"Rack\", \"wrong\", \"staff\", \"item\", \"rib\", \"barbeque\", \"fault\", \"07/22/2017\", \"open\", \"mountain\", \"Arrowhead\", \"error\", \"promise\", \"close\", \"correct\", \"sell\", \"today\", \"wall\", \"people\", \"waiter\", \"find\", \"customer\", \"think\", \"like\", \"good\", \"\\n\\n\", \"friendly\", \"ok\", \"nice\", \"know\", \"love\", \"way\", \"time\", \" \", \"place\", \"sauce\", \"great\", \"service\", \"food\", \"special\", \"restaurant\", \"come\", \"tell\", \"leak\", \"start\", \"sample\", \"problem\", \"teacher\", \"fix\", \"oil\", \"bike\", \"call\", \"sampling\", \"flavour\", \"buy\", \"mean\", \"room\", \"1000\", \"biggie\", \"home\", \"literally\", \"walk\", \"Greek\", \"main\", \"greek\", \"Salad\", \"past\", \"Restaurant\", \"chili\", \"cold\", \"ridiculous\", \"vanilla\", \"Gelato\", \"pay\", \"$\", \"lot\", \"\\n\\n\", \"place\", \"get\", \"\\n\", \"experience\", \"come\", \"husband\", \"good\", \"great\", \"time\", \"pretty\", \"go\", \"restaurant\", \"say\", \"lose\", \"feel\", \"noise\", \"part\", \"vehicle\", \"far\", \"technician\", \"hear\", \"Spicy\", \"Shrimp\", \"pakora\", \"respect\", \"gate\", \"complete\", \"certain\", \"frustrating\", \"finally\", \"scheduling\", \"inform\", \"shrimp\", \"version\", \"work\", \"spicy\", \"Thai\", \"cuisine\", \"Hakka\", \"week\", \"issue\", \"car\", \"Chicken\", \"overrate\", \"tee\", \"burger\", \"food\", \"service\", \"\\n\", \"order\", \" \", \"take\", \"need\", \"way\", \"love\", \"sauce\", \"like\", \"good\", \"\\n\\n\", \"server\", \"waitress\", \"flop\", \"end\", \"bottle\", \"bachelorette\", \"flip\", \"hostess\", \"wear\", \"shoe\", \"dress\", \"1/4\", \"leg\", \"burn\", \"thigh\", \"professional\", \"smog\", \"Yelp\", \"Mike\", \"coupon\", \"thank\", \"host\", \"sing\", \"local\", \"water\", \"entertain\", \"crowd\", \"Las\", \"Dino\", \"karaoke\", \"pool\", \"tourist\", \"mix\", \"party\", \"easy\", \"short\", \" \", \"actually\", \"away\", \"look\", \"great\", \"10\", \"night\", \"time\", \"service\", \"get\", \"husband\", \"nice\", \"like\", \"good\", \"\\n\\n\", \"place\", \"feel\", \"tofu\", \"minimum\", \"unlike\", \"delivery\", \"w/\", \"unreasonably\", \"vegetable\", \"value\", \"China\", \"texture\", \"high\", \"Palace\", \"highly\", \"recommend\", \"12\", \"evening\", \"reasonable\", \"special\", \"cold\", \"tasty\", \"amazing\", \"restaurant\", \"find\", \"order\", \"definitely\", \"$\", \"get\", \"cargo\", \"code\", \"ass\", \"food\", \"great\", \"live\", \"Hakka\", \"\\n\\n\"], \"Freq\": [15.0, 4.0, 5.0, 5.0, 7.0, 9.0, 6.0, 3.0, 5.0, 4.0, 2.0, 4.0, 4.0, 2.0, 4.0, 2.0, 9.0, 2.0, 3.0, 3.0, 10.0, 1.0, 1.0, 1.0, 0.0, 0.0, 5.0, 5.0, 2.0, 2.0, 4.807041168212891, 4.803817272186279, 4.030670166015625, 3.253994941711426, 3.25338077545166, 3.252615451812744, 3.251715660095215, 3.252265214920044, 2.4805798530578613, 2.4791603088378906, 2.4787588119506836, 2.4793996810913086, 2.4795403480529785, 2.478487253189087, 2.4781901836395264, 2.477734327316284, 1.7051118612289429, 1.7047839164733887, 1.7052688598632812, 1.7054660320281982, 1.704792857170105, 1.7047014236450195, 1.7045384645462036, 1.7052271366119385, 1.704343318939209, 1.7039715051651, 1.7042287588119507, 1.7047611474990845, 1.7045649290084839, 1.7048875093460083, 2.478883743286133, 4.806826591491699, 3.254293203353882, 7.126588821411133, 8.684706687927246, 11.014334678649902, 3.2559332847595215, 3.25445818901062, 4.033842086791992, 3.254081964492798, 3.2566282749176025, 3.2550671100616455, 4.805854797363281, 5.585233688354492, 4.029356479644775, 3.254448652267456, 3.2608423233032227, 2.486647129058838, 2.4840426445007324, 2.4824302196502686, 2.4820518493652344, 2.481788158416748, 2.481187343597412, 3.674675226211548, 3.674205780029297, 2.2645602226257324, 2.262665271759033, 2.2618160247802734, 2.2624003887176514, 2.2620444297790527, 2.261305093765259, 2.2612111568450928, 1.5568666458129883, 1.5568662881851196, 1.5560667514801025, 1.5554133653640747, 1.5552512407302856, 1.5552300214767456, 1.5550137758255005, 1.5550026893615723, 1.5550106763839722, 1.5544670820236206, 1.554656744003296, 1.5546914339065552, 1.5545337200164795, 1.5544143915176392, 1.5542618036270142, 1.554268717765808, 1.5539772510528564, 1.5543184280395508, 0.8491668105125427, 0.8491668105125427, 0.849166750907898, 2.2607741355895996, 2.970357656478882, 2.2643656730651855, 9.332418441772461, 4.3903937339782715, 2.97145676612854, 2.9706151485443115, 2.2631585597991943, 2.2626075744628906, 2.264099359512329, 2.976760149002075, 2.263399839401245, 2.264677047729492, 1.5602480173110962, 1.5587419271469116, 1.5574971437454224, 1.55717134475708, 1.557065725326538, 1.5565664768218994, 3.5844268798828125, 2.896566390991211, 2.8944740295410156, 2.2059378623962402, 2.206014394760132, 2.205606460571289, 2.2023420333862305, 2.2013027667999268, 2.2012791633605957, 1.5191569328308105, 1.5174236297607422, 1.516620397567749, 1.5169398784637451, 1.516648530960083, 1.5164997577667236, 1.516287922859192, 1.5162017345428467, 1.5147700309753418, 1.5146769285202026, 1.516141653060913, 1.5146785974502563, 1.513946533203125, 1.5125449895858765, 1.5118076801300049, 2.8957419395446777, 2.895362615585327, 2.2058141231536865, 2.200714111328125, 0.8285991549491882, 0.8285990357398987, 2.2088873386383057, 4.268796920776367, 4.271553039550781, 3.584970235824585, 2.893212080001831, 4.9664812088012695, 2.206367254257202, 2.2059590816497803, 2.206392526626587, 2.2017858028411865, 2.2023215293884277, 2.205726146697998, 2.2028138637542725, 2.200348138809204, 1.5191657543182373, 1.5190242528915405, 1.1354504823684692, 1.1354942321777344, 1.1353510618209839, 1.135154366493225, 1.135133981704712, 1.135042428970337, 1.135075330734253, 1.1348828077316284, 1.652212381362915, 0.6202213764190674, 0.620221734046936, 0.6202211380004883, 0.6202203631401062, 0.6202041506767273, 0.6201955080032349, 0.6201933026313782, 0.6201921105384827, 0.6201893091201782, 0.6201891899108887, 0.619903028011322, 0.6198995113372803, 0.6198908090591431, 0.6198772192001343, 0.6198769211769104, 0.6198693513870239, 0.619833767414093, 0.6198292374610901, 0.61982661485672, 0.6197954416275024, 0.619785487651825, 1.136030673980713, 1.135572910308838, 1.1367063522338867, 1.1352256536483765, 4.754332065582275, 1.1358519792556763, 1.1350154876708984, 1.1360918283462524, 2.171208143234253, 1.1376086473464966, 1.135561227798462, 2.1698877811431885, 1.6536840200424194, 1.1379570960998535, 1.1380847692489624, 1.1364847421646118, 1.1391562223434448, 1.1403194665908813, 0.6237009763717651, 0.6219386458396912, 0.6219189763069153, 0.4516952633857727, 0.4514749348163605, 0.2463579773902893, 0.2463529407978058, 0.24634584784507751, 0.2463388293981552, 0.24633903801441193, 0.24632655084133148, 0.2463030368089676, 0.2462991178035736, 0.24628008902072906, 0.2462395280599594, 0.2464548796415329, 0.24643081426620483, 0.2466479390859604, 0.24646522104740143, 0.2464338093996048, 0.4522285461425781, 0.24630410969257355, 0.2466123402118683, 0.24643747508525848, 0.45212340354919434, 0.24716493487358093, 0.4541383981704712, 0.24691101908683777, 0.24693875014781952, 0.24673175811767578, 0.041287798434495926, 0.041282929480075836, 0.04127960279583931, 0.24904204905033112, 0.24757279455661774, 0.04195272922515869, 0.042868100106716156, 0.04652446135878563], \"Total\": [15.0, 4.0, 5.0, 5.0, 7.0, 9.0, 6.0, 3.0, 5.0, 4.0, 2.0, 4.0, 4.0, 2.0, 4.0, 2.0, 9.0, 2.0, 3.0, 3.0, 10.0, 1.0, 1.0, 1.0, 0.0, 0.0, 5.0, 5.0, 2.0, 2.0, 5.233912944793701, 5.230947494506836, 4.4587812423706055, 3.6801609992980957, 3.679917335510254, 3.6810150146484375, 3.6805663108825684, 3.6813833713531494, 2.906296730041504, 2.904979705810547, 2.9046592712402344, 2.90543794631958, 2.9060680866241455, 2.905405282974243, 2.905409097671509, 2.9052324295043945, 2.130096197128296, 2.1298224925994873, 2.1304478645324707, 2.130697250366211, 2.1299126148223877, 2.1298344135284424, 2.129718065261841, 2.130643367767334, 2.129638910293579, 2.129342794418335, 2.1298534870147705, 2.1306769847869873, 2.130554676055908, 2.131040334701538, 3.110944986343384, 6.439213752746582, 4.389242172241211, 11.368775367736816, 15.046965599060059, 23.21732521057129, 4.888071060180664, 5.057835102081299, 6.873790740966797, 5.595296382904053, 5.749147891998291, 5.7524003982543945, 10.800639152526855, 15.494097709655762, 9.91004467010498, 6.456460952758789, 9.458508491516113, 9.304779052734375, 7.250254154205322, 4.008573532104492, 5.42386531829834, 5.7189836502075195, 4.8037214279174805, 4.1163506507873535, 4.116399765014648, 2.70237398147583, 2.7018702030181885, 2.7009503841400146, 2.7017982006073, 2.70170259475708, 2.70163893699646, 2.7016546726226807, 1.9946792125701904, 1.9946790933609009, 1.9944663047790527, 1.9941577911376953, 1.9940006732940674, 1.9942419528961182, 1.9941846132278442, 1.9941813945770264, 1.9943552017211914, 1.9939056634902954, 1.9944369792938232, 1.9945400953292847, 1.9944590330123901, 1.9943867921829224, 1.9942476749420166, 1.9942986965179443, 1.994095802307129, 2.2010531425476074, 1.2869787216186523, 1.2869789600372314, 1.2869789600372314, 3.476785898208618, 5.683156490325928, 4.16730260848999, 23.21732521057129, 9.91004467010498, 6.030305862426758, 7.635587215423584, 5.45595121383667, 5.7189836502075195, 6.061697959899902, 15.046965599060059, 9.458508491516113, 10.800639152526855, 4.236730098724365, 5.012129783630371, 5.42386531829834, 5.5275068283081055, 2.77020263671875, 4.062004566192627, 4.031178951263428, 3.3411507606506348, 3.340906858444214, 2.6503958702087402, 2.6505579948425293, 2.650512933731079, 2.647325038909912, 2.6464691162109375, 2.646517515182495, 1.9603999853134155, 1.9604442119598389, 1.9599113464355469, 1.9604558944702148, 1.9602549076080322, 1.960080623626709, 1.960099697113037, 1.9600526094436646, 1.9582033157348633, 1.9580966234207153, 1.9600820541381836, 1.9583697319030762, 1.95793616771698, 1.9564553499221802, 1.9563993215560913, 4.047975540161133, 4.048483371734619, 3.167268991470337, 3.353625535964966, 1.269841194152832, 1.269841194152832, 3.426405191421509, 7.250254154205322, 9.304779052734375, 7.635587215423584, 5.749141693115234, 15.494097709655762, 4.065306186676025, 4.0647873878479, 5.7524003982543945, 5.749147891998291, 6.456460952758789, 11.368775367736816, 15.046965599060059, 23.21732521057129, 2.7356040477752686, 3.5112721920013428, 1.6130521297454834, 1.6131656169891357, 1.6132134199142456, 1.6130324602127075, 1.613085389137268, 1.6130759716033936, 1.6132365465164185, 1.613187313079834, 2.8203039169311523, 1.09620201587677, 1.0962027311325073, 1.0962024927139282, 1.0962028503417969, 1.096211314201355, 1.0962144136428833, 1.0962151288986206, 1.0962156057357788, 1.0962164402008057, 1.0962164402008057, 1.0963239669799805, 1.096325397491455, 1.096328616142273, 1.096333622932434, 1.0963332653045654, 1.0963363647460938, 1.0963492393493652, 1.0963506698608398, 1.0963518619537354, 1.0963634252548218, 1.0963671207427979, 2.320476770401001, 2.32025146484375, 2.389131546020508, 2.3883769512176514, 15.494097709655762, 2.9942777156829834, 2.993732213973999, 3.010580539703369, 9.458508491516113, 3.1636593341827393, 3.1646125316619873, 10.800639152526855, 9.304779052734375, 6.030305862426758, 6.061697959899902, 6.873790740966797, 11.368775367736816, 15.046965599060059, 23.21732521057129, 9.91004467010498, 4.062004566192627, 0.9919467568397522, 0.9923967123031616, 0.7857365012168884, 0.7857367992401123, 0.7857842445373535, 0.7857919931411743, 0.785801112651825, 0.7858176827430725, 0.7858917713165283, 0.7858990430831909, 0.7859690189361572, 0.7860574126243591, 1.4931440353393555, 1.493199110031128, 1.5606404542922974, 1.5611927509307861, 1.5612255334854126, 4.008573532104492, 2.2010531425476074, 2.251315116882324, 2.3372445106506348, 5.42386531829834, 3.110944986343384, 5.749141693115234, 4.4249701499938965, 5.683156490325928, 6.030305862426758, 1.0962074995040894, 1.096169352531433, 1.0961576700210571, 7.250254154205322, 9.458508491516113, 1.267917275428772, 1.9563993215560913, 23.21732521057129], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.695400238037109, -4.696100234985352, -4.871600151062012, -5.085599899291992, -5.0858001708984375, -5.085999965667725, -5.086299896240234, -5.08620023727417, -5.35699987411499, -5.357600212097168, -5.357699871063232, -5.357500076293945, -5.357399940490723, -5.357900142669678, -5.357999801635742, -5.3582000732421875, -5.731900215148926, -5.732100009918213, -5.731800079345703, -5.7316999435424805, -5.732100009918213, -5.732100009918213, -5.7322001457214355, -5.731800079345703, -5.7322998046875, -5.732500076293945, -5.732399940490723, -5.732100009918213, -5.7322001457214355, -5.73199987411499, -5.357699871063232, -4.695499897003174, -5.0854997634887695, -4.301700115203857, -4.103899955749512, -3.866300106048584, -5.085000038146973, -5.0854997634887695, -4.870800018310547, -5.085599899291992, -5.084799766540527, -5.085299968719482, -4.695700168609619, -4.545400142669678, -4.8719000816345215, -5.0854997634887695, -5.083499908447266, -5.354599952697754, -5.355599880218506, -5.356299877166748, -5.356400012969971, -5.356500148773193, -5.356800079345703, -4.589900016784668, -4.590099811553955, -5.073999881744385, -5.074900150299072, -5.075200080871582, -5.074999809265137, -5.075099945068359, -5.075500011444092, -5.075500011444092, -5.448699951171875, -5.448699951171875, -5.44920015335083, -5.449699878692627, -5.44980001449585, -5.44980001449585, -5.449900150299072, -5.449900150299072, -5.449900150299072, -5.450300216674805, -5.450200080871582, -5.450099945068359, -5.450200080871582, -5.450300216674805, -5.450399875640869, -5.450399875640869, -5.4506001472473145, -5.450399875640869, -6.054900169372559, -6.054900169372559, -6.054900169372559, -5.075699806213379, -4.802700042724609, -5.074100017547607, -3.657900094985962, -4.4120001792907715, -4.8024001121521, -4.802599906921387, -5.0746002197265625, -5.074900150299072, -5.07420015335083, -4.800600051879883, -5.07450008392334, -5.073999881744385, -5.446599960327148, -5.447500228881836, -5.448299884796143, -5.448500156402588, -5.448599815368652, -5.44890022277832, -4.5406999588012695, -4.753799915313721, -4.754499912261963, -5.026199817657471, -5.026100158691406, -5.026299953460693, -5.0278000831604, -5.028299808502197, -5.028299808502197, -5.399199962615967, -5.400300025939941, -5.4008002281188965, -5.400599956512451, -5.4008002281188965, -5.400899887084961, -5.401000022888184, -5.401100158691406, -5.4019999504089355, -5.402100086212158, -5.401100158691406, -5.402100086212158, -5.402599811553955, -5.403500080108643, -5.4039998054504395, -4.7540998458862305, -4.754199981689453, -5.026199817657471, -5.028500080108643, -6.005300045013428, -6.005300045013428, -5.024799823760986, -4.366000175476074, -4.365300178527832, -4.540599822998047, -4.754899978637695, -4.214600086212158, -5.026000022888184, -5.026100158691406, -5.025899887084961, -5.0279998779296875, -5.0278000831604, -5.026299953460693, -5.027599811553955, -5.02869987487793, -5.399099826812744, -5.399199962615967, -4.9421000480651855, -4.9421000480651855, -4.942200183868408, -4.942399978637695, -4.942399978637695, -4.942500114440918, -4.942500114440918, -4.942599773406982, -4.5671000480651855, -5.546899795532227, -5.546899795532227, -5.546899795532227, -5.546899795532227, -5.546899795532227, -5.546899795532227, -5.546899795532227, -5.546899795532227, -5.546899795532227, -5.546899795532227, -5.547399997711182, -5.547399997711182, -5.547399997711182, -5.547399997711182, -5.547399997711182, -5.547399997711182, -5.547500133514404, -5.547500133514404, -5.547500133514404, -5.547500133514404, -5.547599792480469, -4.9415998458862305, -4.941999912261963, -4.940999984741211, -4.942299842834473, -3.5100998878479004, -4.941800117492676, -4.942500114440918, -4.9415998458862305, -4.293900012969971, -4.940199851989746, -4.941999912261963, -4.29449987411499, -4.566199779510498, -4.939899921417236, -4.939799785614014, -4.941199779510498, -4.938899993896484, -4.937900066375732, -5.541299819946289, -5.544099807739258, -5.544099807739258, -4.428899765014648, -4.4293999671936035, -5.035099983215332, -5.035099983215332, -5.035200119018555, -5.035200119018555, -5.035200119018555, -5.035299777984619, -5.035399913787842, -5.035399913787842, -5.035399913787842, -5.035600185394287, -5.0346999168396, -5.034800052642822, -5.033999919891357, -5.0346999168396, -5.034800052642822, -4.427700042724609, -5.035299777984619, -5.03410005569458, -5.034800052642822, -4.427999973297119, -5.031899929046631, -4.423500061035156, -5.032899856567383, -5.032800197601318, -5.033599853515625, -6.821300029754639, -6.821499824523926, -6.821499824523926, -5.0243000984191895, -5.030200004577637, -6.8053998947143555, -6.78380012512207, -6.701900005340576], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9085000157356262, 0.9083999991416931, 0.8927000164985657, 0.8705000281333923, 0.8704000115394592, 0.8698999881744385, 0.869700014591217, 0.869700014591217, 0.8352000117301941, 0.835099995136261, 0.835099995136261, 0.8349999785423279, 0.8349000215530396, 0.8346999883651733, 0.8345999717712402, 0.8343999981880188, 0.7710999846458435, 0.7710000276565552, 0.7710000276565552, 0.7710000276565552, 0.7710000276565552, 0.7710000276565552, 0.7709000110626221, 0.7709000110626221, 0.770799994468689, 0.770799994468689, 0.7706999778747559, 0.7706000208854675, 0.7705000042915344, 0.7705000042915344, 0.7664999961853027, 0.701200008392334, 0.6944000124931335, 0.5266000032424927, 0.4440000057220459, 0.24789999425411224, 0.5873000025749207, 0.5526999831199646, 0.46059998869895935, 0.45159998536109924, 0.4253000020980835, 0.42419999837875366, 0.18379999697208405, -0.02669999934732914, 0.09369999915361404, 0.3086000084877014, -0.07129999995231628, -0.32600000500679016, -0.07750000059604645, 0.5144000053405762, 0.211899995803833, 0.15880000591278076, 0.3330000042915344, 1.2541999816894531, 1.25409996509552, 1.190999984741211, 1.1902999877929688, 1.1902999877929688, 1.1901999711990356, 1.1900999546051025, 1.1898000240325928, 1.1897000074386597, 1.1198999881744385, 1.1198999881744385, 1.1195000410079956, 1.1191999912261963, 1.1191999912261963, 1.1190999746322632, 1.11899995803833, 1.11899995803833, 1.118899941444397, 1.1187000274658203, 1.1186000108718872, 1.1186000108718872, 1.118499994277954, 1.118499994277954, 1.118399977684021, 1.118399977684021, 1.118299961090088, 1.0197999477386475, 0.9519000053405762, 0.9519000053405762, 0.9519000053405762, 0.9373000264167786, 0.7189000248908997, 0.7577000260353088, 0.4562999904155731, 0.553600013256073, 0.6600000262260437, 0.4237000048160553, 0.4878000020980835, 0.44040000438690186, 0.3828999996185303, -0.2526000142097473, -0.062300000339746475, -0.19449999928474426, 0.36880001425743103, 0.1996999979019165, 0.11999999731779099, 0.10080000013113022, 0.7915999889373779, 0.40849998593330383, 1.3243000507354736, 1.2990000247955322, 1.2984000444412231, 1.2582999467849731, 1.2582000494003296, 1.2581000328063965, 1.2577999830245972, 1.257599949836731, 1.257599949836731, 1.1868000030517578, 1.1857000589370728, 1.1854000091552734, 1.1852999925613403, 1.1851999759674072, 1.1851999759674072, 1.1850999593734741, 1.184999942779541, 1.184999942779541, 1.184999942779541, 1.184999942779541, 1.1849000453948975, 1.1845999956130981, 1.184499979019165, 1.184000015258789, 1.1067999601364136, 1.106600046157837, 1.0800000429153442, 1.0204999446868896, 1.0148999691009521, 1.0148999691009521, 1.0027999877929688, 0.9121000170707703, 0.6632999777793884, 0.685699999332428, 0.7551000118255615, 0.30410000681877136, 0.8306999802589417, 0.8306000232696533, 0.483599990606308, 0.4819999933242798, 0.3662000000476837, -0.1979999989271164, -0.4796000123023987, -0.9144999980926514, 0.853600025177002, 0.6039000153541565, 1.8387999534606934, 1.8387999534606934, 1.8387000560760498, 1.8386000394821167, 1.8385000228881836, 1.8385000228881836, 1.8384000062942505, 1.8382999897003174, 1.6552000045776367, 1.6203999519348145, 1.6203999519348145, 1.6203999519348145, 1.6203999519348145, 1.6203999519348145, 1.6203999519348145, 1.6203999519348145, 1.6203999519348145, 1.620300054550171, 1.620300054550171, 1.6197999715805054, 1.6197999715805054, 1.6197999715805054, 1.6196999549865723, 1.6196999549865723, 1.6196999549865723, 1.6196999549865723, 1.6196000576019287, 1.6196000576019287, 1.6196000576019287, 1.6196000576019287, 1.4757000207901, 1.4753999710083008, 1.447100043296814, 1.4462000131607056, 1.0084999799728394, 1.2206000089645386, 1.2201000452041626, 1.215399980545044, 0.7182999849319458, 1.167099952697754, 1.1649999618530273, 0.5849999785423279, 0.46239998936653137, 0.5224000215530396, 0.517300009727478, 0.3901999890804291, -0.11060000211000443, -0.38989999890327454, -1.4270000457763672, -0.578499972820282, 0.3133000135421753, 2.8382999897003174, 2.837399959564209, 2.465100049972534, 2.465100049972534, 2.4649999141693115, 2.4649999141693115, 2.4649999141693115, 2.464900016784668, 2.4646999835968018, 2.4646999835968018, 2.4644999504089355, 2.464200019836426, 1.8235000371932983, 1.8234000205993652, 1.7800999879837036, 1.7790000438690186, 1.7788000106811523, 1.4429999589920044, 1.4348000288009644, 1.4134999513626099, 1.3753000497817993, 1.1404000520706177, 1.0923000574111938, 1.0865999460220337, 0.7390000224113464, 0.4887999892234802, 0.4287000000476837, 0.3458999991416931, 0.3458000123500824, 0.3458000123500824, 0.25380000472068787, -0.017999999225139618, 0.21639999747276306, -0.19580000638961792, -2.5876998901367188]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 3, 4, 1, 3, 4, 1, 2, 4, 1, 4, 1, 4, 2, 1, 1, 2, 3, 4, 2, 2, 3, 4, 4, 1, 1, 2, 2, 3, 3, 3, 1, 4, 3, 4, 1, 4, 3, 4, 4, 1, 2, 2, 1, 4, 1, 3, 4, 2, 2, 3, 4, 4, 3, 2, 1, 4, 2, 1, 2, 3, 3, 1, 1, 4, 4, 3, 1, 3, 4, 1, 2, 3, 3, 4, 1, 4, 4, 4, 1, 1, 1, 1, 2, 3, 4, 1, 3, 1, 1, 2, 4, 3, 1, 2, 2, 4, 4, 1, 3, 1, 3, 4, 3, 3, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 2, 2, 4, 4, 1, 2, 4, 3, 2, 3, 1, 4, 1, 2, 3, 4, 2, 4, 1, 2, 3, 4, 2, 3, 4, 2, 3, 4, 1, 2, 1, 2, 3, 1, 3, 2, 2, 2, 4, 1, 2, 3, 1, 3, 4, 1, 4, 3, 2, 1, 3, 1, 1, 2, 3, 4, 3, 3, 3, 2, 4, 2, 1, 2, 1, 1, 2, 3, 4, 4, 1, 2, 3, 1, 2, 4, 1, 1, 2, 3, 1, 2, 3, 1, 2, 2, 2, 2, 1, 2, 3, 1, 2, 3, 4, 3, 1, 1, 3, 1, 2, 3, 4, 4, 1, 4, 3, 4, 4, 1, 3, 3, 1, 2, 1, 2, 3, 1, 3, 2, 3, 3, 1, 3, 4, 4, 4, 1, 2, 1, 2, 3, 4, 1, 4, 1, 2, 3, 3, 1, 1, 3, 2, 1, 4, 1, 3, 4, 2, 3, 1, 3, 1], \"Freq\": [0.1309656947851181, 0.3928970992565155, 0.5238627791404724, 0.47378411889076233, 0.3876415491104126, 0.08614256978034973, 0.043071284890174866, 0.38724422454833984, 0.3227035403251648, 0.3227035403251648, 0.3519171178340912, 0.5278756618499756, 0.1759585589170456, 0.9390453696250916, 0.9122406244277954, 0.632179319858551, 0.3160896599292755, 1.0028873682022095, 0.6407625675201416, 0.9390056729316711, 0.29818475246429443, 0.5963695049285889, 0.9121169447898865, 0.7770134806632996, 1.0027892589569092, 1.0222861766815186, 0.912118136882782, 0.9122292995452881, 0.6884729862213135, 0.6885489225387573, 1.0028587579727173, 1.002814531326294, 0.7557238936424255, 0.7554795742034912, 1.0214837789535522, 0.8150919675827026, 0.9122297167778015, 0.6679407358169556, 0.3339703679084778, 0.8557084798812866, 0.9122775197029114, 0.6680624485015869, 0.33403122425079346, 0.6199503540992737, 0.6884130835533142, 1.0029162168502808, 0.7402913570404053, 0.8152356743812561, 0.6198807954788208, 0.29185107350349426, 0.5837021470069885, 0.912240207195282, 1.0027744770050049, 0.7402870655059814, 0.6314588189125061, 0.31572940945625305, 0.9122360348701477, 1.0201709270477295, 1.0029608011245728, 0.9386836290359497, 0.912267804145813, 0.9086559414863586, 0.3497124910354614, 0.3497124910354614, 0.1748562455177307, 1.0204542875289917, 0.8149110674858093, 0.9391263127326965, 0.9122285842895508, 0.9121288061141968, 1.022256851196289, 0.7764922976493835, 0.15529845654964447, 0.15529845654964447, 0.45198044180870056, 0.22599022090435028, 0.45198044180870056, 0.35457172989845276, 0.7091434597969055, 0.4185621440410614, 0.4185621440410614, 0.6198991537094116, 0.9121314287185669, 0.9390401244163513, 0.6405358910560608, 0.6881609559059143, 0.36657220125198364, 0.36657220125198364, 0.18328610062599182, 0.18328610062599182, 0.8149926066398621, 0.7546042799949646, 0.9389247298240662, 0.4923677444458008, 0.4923677444458008, 0.2461838722229004, 1.0203661918640137, 0.6428914666175842, 0.7402477264404297, 1.0026675462722778, 0.6199299693107605, 0.6199427843093872, 0.27585241198539734, 0.5517048239707947, 0.613739013671875, 0.2045796811580658, 0.2045796811580658, 1.0202754735946655, 1.0201770067214966, 0.49748721718788147, 0.33165812492370605, 0.16582906246185303, 0.39903196692466736, 0.39903196692466736, 0.19951598346233368, 0.5981272459030151, 0.19937574863433838, 0.13291716575622559, 0.06645858287811279, 0.3171747326850891, 0.21144983172416687, 0.21144983172416687, 0.21144983172416687, 1.002778172492981, 0.7545709013938904, 0.6697277426719666, 1.002917766571045, 0.9121391177177429, 0.6199336051940918, 0.3299405574798584, 0.3299405574798584, 0.1649702787399292, 1.0203807353973389, 0.24700607359409332, 0.7410182356834412, 0.6883721351623535, 0.9121159315109253, 0.5361646413803101, 0.17872154712677002, 0.17872154712677002, 0.17872154712677002, 0.9717345237731934, 0.9122400283813477, 0.6157215237617493, 0.08796022087335587, 0.17592044174671173, 0.08796022087335587, 1.0028303861618042, 0.7886949777603149, 0.9121352434158325, 0.3321618437767029, 0.3321618437767029, 0.3321618437767029, 0.3609844148159027, 0.7219688296318054, 0.2399633824825287, 0.4799267649650574, 0.2399633824825287, 0.5218164324760437, 0.3478776514530182, 1.0027374029159546, 1.0029296875, 0.4309459328651428, 0.4309459328651428, 0.9386598467826843, 0.49203065037727356, 0.49203065037727356, 0.5819205641746521, 0.29096028208732605, 0.14548014104366302, 0.6319888830184937, 0.3159944415092468, 0.992265522480011, 0.7402739524841309, 0.5931391716003418, 0.39542609453201294, 0.9387697577476501, 0.17393900454044342, 0.17393900454044342, 0.5218170285224915, 0.17393900454044342, 0.7875000238418579, 0.7557100653648376, 0.8978942036628723, 0.43098777532577515, 0.43098777532577515, 1.0028845071792603, 0.28762197494506836, 0.5752439498901367, 0.9387226700782776, 0.4036308825016022, 0.4036308825016022, 0.10090772062540054, 0.10090772062540054, 0.9121063351631165, 0.4720621705055237, 0.4720621705055237, 0.23603108525276184, 0.9558497667312622, 0.740227997303009, 0.9122328758239746, 0.9390914440155029, 0.6405224204063416, 0.6697030663490295, 1.0202000141143799, 0.3687407076358795, 0.3687407076358795, 0.18437035381793976, 0.6883712410926819, 0.7770135998725891, 1.0030087232589722, 0.740090012550354, 1.0026675462722778, 0.46465083956718445, 0.15488360822200775, 0.3097672164440155, 0.3618267774581909, 0.3618267774581909, 0.18091338872909546, 0.18091338872909546, 1.020356297492981, 0.9392569661140442, 0.3655499815940857, 0.7310999631881714, 0.2149433046579361, 0.10747165232896805, 0.4298866093158722, 0.2149433046579361, 0.6198908090591431, 0.4186943769454956, 0.4186943769454956, 1.0213444232940674, 0.9121379256248474, 0.9122303128242493, 0.49893060326576233, 0.24946530163288116, 1.0212576389312744, 0.6882151365280151, 0.9717229008674622, 0.8151817321777344, 0.4919678568840027, 0.4919678568840027, 0.444184809923172, 0.444184809923172, 0.7404801249504089, 0.7545580863952637, 0.7875000238418579, 0.4163438677787781, 0.4163438677787781, 0.20817193388938904, 0.9122285842895508, 0.9122399091720581, 0.6834892630577087, 0.22782976925373077, 0.46293556690216064, 0.18517422676086426, 0.18517422676086426, 0.18517422676086426, 0.939031720161438, 0.9121032357215881, 0.955308198928833, 0.7770134806632996, 0.8979597687721252, 1.0214000940322876, 0.9385087490081787, 0.569594144821167, 0.569594144821167, 1.003056526184082, 0.9386687874794006, 0.912131130695343, 0.5215213894844055, 0.3476809561252594, 0.6198719143867493, 0.24703706800937653, 0.7411112189292908, 0.8971061110496521, 1.0203654766082764, 0.6883643865585327], \"Term\": [\"\\n\", \"\\n\", \"\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \" \", \" \", \" \", \"$\", \"$\", \"$\", \"07/22/2017\", \"1/4\", \"10\", \"10\", \"1000\", \"12\", \"Arrowhead\", \"Chicken\", \"Chicken\", \"Dino\", \"Gelato\", \"Greek\", \"Hakka\", \"Las\", \"Mike\", \"Nordstrom\", \"Rack\", \"Restaurant\", \"Salad\", \"Shrimp\", \"Spicy\", \"Thai\", \"Troutman\", \"Yelp\", \"actually\", \"actually\", \"amazing\", \"ass\", \"away\", \"away\", \"bachelorette\", \"barbeque\", \"biggie\", \"bike\", \"bootie\", \"bottle\", \"burger\", \"burger\", \"burn\", \"buy\", \"call\", \"car\", \"car\", \"cargo\", \"certain\", \"chili\", \"close\", \"code\", \"cold\", \"come\", \"come\", \"come\", \"complete\", \"cook\", \"correct\", \"coupon\", \"crowd\", \"cuisine\", \"customer\", \"customer\", \"customer\", \"definitely\", \"definitely\", \"definitely\", \"dress\", \"dress\", \"easy\", \"easy\", \"end\", \"entertain\", \"error\", \"evening\", \"excellent\", \"experience\", \"experience\", \"experience\", \"experience\", \"family\", \"far\", \"fault\", \"feel\", \"feel\", \"feel\", \"finally\", \"find\", \"fix\", \"flavour\", \"flip\", \"flop\", \"food\", \"food\", \"friendly\", \"friendly\", \"friendly\", \"frustrating\", \"gate\", \"get\", \"get\", \"get\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"great\", \"great\", \"great\", \"great\", \"greek\", \"hear\", \"highly\", \"home\", \"host\", \"hostess\", \"husband\", \"husband\", \"husband\", \"inform\", \"issue\", \"issue\", \"item\", \"karaoke\", \"know\", \"know\", \"know\", \"know\", \"leak\", \"leg\", \"like\", \"like\", \"like\", \"like\", \"literally\", \"live\", \"local\", \"look\", \"look\", \"look\", \"lose\", \"lose\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"main\", \"mean\", \"mix\", \"mix\", \"mountain\", \"need\", \"need\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"noise\", \"oil\", \"ok\", \"ok\", \"open\", \"order\", \"order\", \"order\", \"order\", \"overrate\", \"pakora\", \"part\", \"party\", \"party\", \"past\", \"pay\", \"pay\", \"people\", \"place\", \"place\", \"place\", \"place\", \"pool\", \"pretty\", \"pretty\", \"pretty\", \"price\", \"problem\", \"professional\", \"promise\", \"reasonable\", \"recommend\", \"respect\", \"restaurant\", \"restaurant\", \"restaurant\", \"rib\", \"ridiculous\", \"room\", \"sample\", \"sampling\", \"sauce\", \"sauce\", \"sauce\", \"say\", \"say\", \"say\", \"say\", \"scheduling\", \"sell\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"shoe\", \"short\", \"short\", \"shrimp\", \"sing\", \"smog\", \"special\", \"special\", \"spicy\", \"staff\", \"start\", \"store\", \"take\", \"take\", \"tasty\", \"tasty\", \"teacher\", \"technician\", \"tee\", \"tell\", \"tell\", \"tell\", \"thank\", \"thigh\", \"think\", \"think\", \"time\", \"time\", \"time\", \"time\", \"today\", \"tourist\", \"try\", \"vanilla\", \"vehicle\", \"version\", \"waiter\", \"waitress\", \"waitress\", \"walk\", \"wall\", \"water\", \"way\", \"way\", \"wear\", \"week\", \"week\", \"wine\", \"work\", \"wrong\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1, 5, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1459619538991716562798368025\", ldavis_el1459619538991716562798368025_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1459619538991716562798368025\", ldavis_el1459619538991716562798368025_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1459619538991716562798368025\", ldavis_el1459619538991716562798368025_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.052502 -0.127984       1        1  37.023750\n",
       "1     -0.143803  0.014704       2        1  25.468954\n",
       "0      0.086450  0.100517       3        1  23.649954\n",
       "4      0.015194  0.010817       4        1  11.192327\n",
       "3     -0.010344  0.001946       5        1   2.665019, topic_info=           Term       Freq      Total Category  logprob  loglift\n",
       "0                15.000000  15.000000  Default  30.0000  30.0000\n",
       "151     special   4.000000   4.000000  Default  29.0000  29.0000\n",
       "150  restaurant   5.000000   5.000000  Default  28.0000  28.0000\n",
       "6         order   5.000000   5.000000  Default  27.0000  27.0000\n",
       "101        food   7.000000   7.000000  Default  26.0000  26.0000\n",
       "..          ...        ...        ...      ...      ...      ...\n",
       "101        food   0.249042   7.250254   Topic5  -5.0243   0.2538\n",
       "60        great   0.247573   9.458508   Topic5  -5.0302  -0.0180\n",
       "602        live   0.041953   1.267917   Topic5  -6.8054   0.2164\n",
       "580       Hakka   0.042868   1.956399   Topic5  -6.7838  -0.1958\n",
       "10         \\n\\n   0.046524  23.217325   Topic5  -6.7019  -2.5877\n",
       "\n",
       "[264 rows x 6 columns], token_table=      Topic      Freq   Term\n",
       "term                        \n",
       "160       1  0.130966     \\n\n",
       "160       2  0.392897     \\n\n",
       "160       3  0.523863     \\n\n",
       "10        1  0.473784   \\n\\n\n",
       "10        2  0.387642   \\n\\n\n",
       "...     ...       ...    ...\n",
       "493       2  0.247037   week\n",
       "493       3  0.741111   week\n",
       "721       1  0.897106   wine\n",
       "708       3  1.020365   work\n",
       "445       1  0.688364  wrong\n",
       "\n",
       "[278 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 1, 5, 4])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(lda, corpus, dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above shows us that there are 4 main topics because there is some overlab going on\n",
    "between topic 4 and topic 5. There are 4 main sections which tells us that there probably are 4 main topics. \n",
    "\n",
    "Should probably retrain LDA model with 4 topics to avoid topic overlap. \n",
    "\n",
    "Topic 1 represent words that appear most often in almost all the documents. \n",
    "Topics 4 and 5 represent words that appear least often in almost all the documents. \n",
    "\n",
    "The red bar represent the appearance of a particular word in the selected topic vs\n",
    "blue bar which represent the occurane of the same word in the entire document. \n",
    "\n",
    "red bar = estimated word frequency withing topic\n",
    "\n",
    "blue bar = frequency of word in entire document. \n",
    "\n",
    "Sliding the relevance metric to 0 shows numbers the words that are \n",
    "relevant to just the topic selected\n",
    "but doesn't appear in other topics. \n",
    "\n",
    "Sliding the relevance metric to any number below 1 focuses in on words \n",
    "relevent to that particular topic, could be present in other topics, but more rare. \n",
    "\n",
    "So, the relevance metric can be used to \n",
    "* find words that are specific to a particular topic,\n",
    "* figure out words that appear most often in a particular topic, \n",
    "* figure out which words appear most often in the entire document, \n",
    "* figure out how many unique topics there are, and many other things. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "Unit4-NLP (Python3)",
   "language": "python",
   "name": "unit4-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
